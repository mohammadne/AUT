{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we start, we first specify our attributes: (in attributes.py file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\n",
    "    ['low', 'med', 'high', 'vhigh'],  # buying\n",
    "    ['low', 'med', 'high', 'vhigh'],  # maint\n",
    "    ['2', '3', '4', '5more'],  # doors\n",
    "    ['2', '4', 'more'],  # persons\n",
    "    ['small', 'med', 'big'],  # lug_boot\n",
    "    ['low', 'med', 'high'],  # safety\n",
    "\n",
    "    ['unacc', 'acc', 'good', 'vgood'],  # label\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we create an Object class to map raw data into an Object (in object.py file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attributes import attributes\n",
    "\n",
    "class Object:\n",
    "    def __init__(self, splits) -> None:\n",
    "        self.attributes = []\n",
    "        for index in range(len(attributes)):\n",
    "            value = attributes[index].index(splits[index])\n",
    "            self.attributes.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the main logic of application will be inside the Node class (in node.py file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in node.py we first import our required codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "from objects import Object\n",
    "from attributes import attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we introduce `Node` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,  objects: list[Object], max_depth: int) -> None:\n",
    "        self.objects = objects\n",
    "        self.max_depth = max_depth\n",
    "        self.label = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the `split` function we recursievly split the passed objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def split(self, available_attributes: list[int]) -> None:\n",
    "        maj = self.majority(self.objects)\n",
    "        we_reach = True\n",
    "\n",
    "        for obj in self.objects:\n",
    "            if (obj.attributes[-1] != maj):\n",
    "                we_reach = False\n",
    "                break\n",
    "\n",
    "        depth = (len(attributes) - 1) - len(available_attributes)\n",
    "        if (we_reach or self.max_depth == depth):\n",
    "            self.label = maj\n",
    "            return\n",
    "\n",
    "        splits = []  # contain all splits of nodes\n",
    "\n",
    "        for attribute in available_attributes:\n",
    "            nodes = []  # contain nodes with this attribute\n",
    "\n",
    "            for attribute_value in range(len(attributes[attribute])):\n",
    "                matches = filter(\n",
    "                    lambda obj: obj.attributes[attribute] == attribute_value, self.objects)\n",
    "                nodes.append(Node(list(matches), self.max_depth))\n",
    "\n",
    "            splits.append(nodes)\n",
    "\n",
    "        available_attributes_for_children = available_attributes.copy()\n",
    "\n",
    "        if (len(available_attributes) == 1):\n",
    "            self.attribute = available_attributes[0]\n",
    "            self.children = splits[0]\n",
    "            del available_attributes_for_children[0]\n",
    "        else:\n",
    "            max_gain_ratio_index = 0\n",
    "            max_gain_ratio = 0\n",
    "            for index in range(len(available_attributes)):\n",
    "                gain_ratio = self.gain_ratio(splits[index])\n",
    "                if (gain_ratio > max_gain_ratio):\n",
    "                    gain_ratio\n",
    "                    max_gain_ratio = gain_ratio\n",
    "                    max_gain_ratio_index = index\n",
    "\n",
    "            self.attribute = available_attributes[max_gain_ratio_index]\n",
    "            self.children = splits[max_gain_ratio_index]\n",
    "\n",
    "            del available_attributes_for_children[max_gain_ratio_index]\n",
    "\n",
    "        for child in self.children:\n",
    "            child.split(available_attributes_for_children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `gain_ratio` method will be responsible for calculating the `gain_ratio` of the self node with possible children nodes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def gain_ratio(self, nodes: list) -> float:\n",
    "        gain_info = self.entropy(self)\n",
    "        for node in nodes:\n",
    "            if (len(node.objects) == 0):\n",
    "                continue\n",
    "\n",
    "            entropy = self.entropy(node)\n",
    "            ratio = len(node.objects) / len(self.objects)\n",
    "            gain_info -= ratio*entropy\n",
    "\n",
    "        split_info = 0\n",
    "        for node in nodes:\n",
    "            if (len(node.objects) == 0):\n",
    "                continue\n",
    "\n",
    "            ratio = len(node.objects) / len(self.objects)\n",
    "            split_info -= ratio * log(ratio, 2)\n",
    "\n",
    "        if (split_info == 0):\n",
    "            return 0\n",
    "\n",
    "        return gain_info / split_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `get_label` function is responsible for getting the class lable for one sample test object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_label(self, obj: Object) -> int:\n",
    "        if (self.label != -1):\n",
    "            return self.label\n",
    "\n",
    "        attribute_value = obj.attributes[self.attribute]\n",
    "        return self.children[attribute_value].get_label(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here `get_training_error_counts` is responsible for recursively calculates number of errors in training objects with calculated `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_training_error_counts(self) -> int:\n",
    "        if (self.label != -1):\n",
    "            matches = filter(\n",
    "                lambda obj: obj.attributes[-1] != self.label, self.objects)\n",
    "            return len(list(matches))\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for child in self.children:\n",
    "            count += child.get_training_error_counts()\n",
    "\n",
    "        return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `majority` is a `static` method for calculating label of the leaf node by calculating majority label class of node objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def majority(objects: list[Object]) -> int:\n",
    "        counts = [0] * len(attributes[-1])\n",
    "\n",
    "        for obj in objects:\n",
    "            counts[obj.attributes[-1]] += 1\n",
    "\n",
    "        max_value, max_index = -1, -1\n",
    "        for index in range(len(counts)):\n",
    "            if (counts[index] > max_value):\n",
    "                max_value = counts[index]\n",
    "                max_index = index\n",
    "\n",
    "        return max_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `entropy` method is also a `static` one which is responsible for calculating entropy for a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def entropy(node) -> float:\n",
    "        entropy = 0.0\n",
    "\n",
    "        for label in range(len(attributes[-1])):\n",
    "            matches = filter(\n",
    "                lambda obj: obj.attributes[-1] == label, node.objects)\n",
    "\n",
    "            ratio = len(list(matches)) / len(node.objects)\n",
    "            if (ratio != 0):\n",
    "                entropy -= ratio * log(ratio, 2)\n",
    "\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at the `main.py` as entrypoint of the application we are inducting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we first import our written files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objects import Object\n",
    "from attributes import attributes\n",
    "from node import Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we are reading the contents of data and convert them into `Object` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./static/car.data\", \"r\")\n",
    "lines = file.readlines()\n",
    "\n",
    "objects = []\n",
    "\n",
    "for line in lines:\n",
    "    splits = line.removesuffix('\\n').split(',')\n",
    "    objects.append(Object(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we are using 90% of the objects as our training set and calculating the training_error and testing_error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = len(attributes) - 1\n",
    "\n",
    "division_number = round(len(objects) * (90/100))\n",
    "trainee_objects = objects[:division_number]\n",
    "test_objects = objects[division_number:]\n",
    "\n",
    "root = Node(trainee_objects, max_depth)\n",
    "available_attributes = list(range(0, max_depth))\n",
    "root.split(available_attributes)\n",
    "\n",
    "training_error = root.get_training_error_counts()\n",
    "print(f'training error: {(training_error/division_number)*100}')\n",
    "\n",
    "testing_error = 0\n",
    "for test_object in test_objects:\n",
    "    label = root.get_label(test_object)\n",
    "    if (test_object.attributes[-1] != label):\n",
    "        testing_error += 1\n",
    "\n",
    "print(f'test error: {(testing_error/(len(objects)-division_number))*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we use k-fold cross validation method as our induction strategy and calculate the average errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_fold(k: int, max_depth: int = len(attributes)-1) -> float:\n",
    "    k_fold_division_count = round(len(objects) / k)\n",
    "    k_fold_training_error = 0\n",
    "    k_fold_testing_error = 0\n",
    "\n",
    "    for index in range(k):\n",
    "        start = k_fold_division_count * index\n",
    "        end = k_fold_division_count*(index+1)\n",
    "        trainee_objects = objects[0:start] + objects[end:]\n",
    "        test_objects = objects[start:end]\n",
    "\n",
    "        root = Node(trainee_objects, max_depth)\n",
    "        available_attributes = list(range(0, len(attributes)-1))\n",
    "        root.split(available_attributes)\n",
    "\n",
    "        k_fold_training_error += root.get_training_error_counts()\n",
    "\n",
    "        for test_object in test_objects:\n",
    "            label = root.get_label(test_object)\n",
    "            if (test_object.attributes[-1] != label):\n",
    "                k_fold_testing_error += 1\n",
    "\n",
    "    return (k_fold_testing_error/(k_fold_division_count))/k\n",
    "\n",
    "\n",
    "print()\n",
    "k_values = [3, 5, 10, 15, 20]\n",
    "for value in k_values:\n",
    "    test_error = k_fold(value) * 100\n",
    "    print(f'test error with {value}-fold cross-validation: {test_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as question asked, we are using k-fold with the provided `k` value above and if we run the application we will see:\n",
    "\n",
    "### running with normal strategy (90% of the data as training) we have:\n",
    "\n",
    "> test error: 35.83815028901734\n",
    "\n",
    "### running with k-fold strategy we have the following result:\n",
    "\n",
    "> test error with 3-fold cross-validation: 31.770833333333332\n",
    ">\n",
    "> test error with 5-fold cross-validation: 24.91329479768786\n",
    ">\n",
    "> test error with 10-fold cross-validation: 23.179190751445084\n",
    ">\n",
    "> test error with 15-fold cross-validation: 14.318840579710146\n",
    ">\n",
    "> test error with 20-fold cross-validation: 10.290697674418606\n",
    "\n",
    "as you can see as we increase the number of folds the value of `test error` will be decresed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_depth_value = 0\n",
    "optimal_depth = float(\"inf\")\n",
    "\n",
    "print()\n",
    "for depth in range(len(attributes) - 1):\n",
    "    k = 10\n",
    "    test_error = k_fold(k, depth+1)\n",
    "    complexity_error = 0.05 * (depth+1) / max_depth\n",
    "    sum_error = (test_error + complexity_error) * 100\n",
    "\n",
    "    if (sum_error < optimal_depth):\n",
    "        optimal_depth = sum_error\n",
    "        optimal_depth_value = depth+1\n",
    "\n",
    "    print(\n",
    "        f'test error with {k}-fold cross-validation and depth: {depth+1}: {sum_error}')\n",
    "\n",
    "print()\n",
    "print(\n",
    "    f'maximum depth is: {max_depth}, optimal depth is: {optimal_depth_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are calculating optimal depth by calculating the complexity of the model:\n",
    "\n",
    "> test error with 10-fold cross-validation and depth: 1: 30.775529865125247\n",
    "\n",
    "> test error with 10-fold cross-validation and depth: 2: 27.62042389210019\n",
    "\n",
    "> test error with 10-fold cross-validation and depth: 3: 28.800578034682083\n",
    "\n",
    "> test error with 10-fold cross-validation and depth: 4: 28.82466281310212\n",
    "\n",
    "> test error with 10-fold cross-validation and depth: 5: 27.46146435452794\n",
    "\n",
    "> test error with 10-fold cross-validation and depth: 6: 28.179190751445088\n",
    "\n",
    "> maximum depth is: 6, optimal depth is: 5\n",
    "\n",
    "as you can see, to avoid model overfitting it's better to calculate model complexity and determine the optimal depth as you can see here that optimal depth is not always the maximum one."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
